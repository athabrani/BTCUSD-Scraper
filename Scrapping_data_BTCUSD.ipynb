{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHDwVo8DM8ZlP2HY2ovxyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athabrani/BTCUSD-Scraper/blob/main/Scrapping_data_BTCUSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapping data dari bitstamp\n"
      ],
      "metadata": {
        "id": "oWBRaf-GcUPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZZhzhuzXWD5",
        "outputId": "60b1e73d-a0dd-4623-fe0b-139862bc98aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data berhasil diexport ke: /btcusd_bitstamp_4h_9000.csv\n",
            "               Timestamp      Open      High       Low     Close      Volume\n",
            "8000 2021-09-29 23:00:00  41687.48  41807.10  41038.49  41194.33  328.773574\n",
            "8001 2021-09-30 03:00:00  41175.95  41566.37  40900.00  41542.90  184.691766\n",
            "8002 2021-09-30 07:00:00  41537.56  43731.58  41427.87  43580.52  374.906749\n",
            "8003 2021-09-30 11:00:00  43621.31  43835.09  43225.00  43290.72  440.090322\n",
            "8004 2021-09-30 15:00:00  43294.61  43460.00  42725.00  43086.49  439.272284\n",
            "Total baris: 9000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from math import ceil\n",
        "import time\n",
        "\n",
        "\n",
        "PAIR = \"btcusd\"\n",
        "URL = f\"https://www.bitstamp.net/api/v2/ohlc/{PAIR}/\"\n",
        "\n",
        "STEP = 4 * 60 * 60          # 4 jam = 14400 detik\n",
        "TOTAL_NEEDED = 9000         # target jumlah candle\n",
        "PER_CALL_LIMIT = 1000       # batas aman per request\n",
        "TIMEOUT = 30                # detik untuk requests\n",
        "\n",
        "\n",
        "all_rows = []\n",
        "remaining = TOTAL_NEEDED\n",
        "\n",
        "\n",
        "end_ts = int(time.time())\n",
        "\n",
        "while remaining > 0:\n",
        "    batch_limit = min(PER_CALL_LIMIT, remaining)\n",
        "    params = {\n",
        "        \"step\": STEP,\n",
        "        \"limit\": batch_limit,\n",
        "        \"end\": end_ts\n",
        "    }\n",
        "    r = requests.get(URL, params=params, timeout=TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    payload = r.json()[\"data\"][\"ohlc\"]\n",
        "\n",
        "    if not payload:\n",
        "        print(\"Tidak ada data yang dikembalikan. Berhenti lebih awal.\")\n",
        "        break\n",
        "\n",
        "\n",
        "    payload.sort(key=lambda x: int(x[\"timestamp\"]))\n",
        "\n",
        "    all_rows.extend(payload)\n",
        "\n",
        "\n",
        "    earliest_ts = int(payload[0][\"timestamp\"])\n",
        "    end_ts = earliest_ts - 1\n",
        "\n",
        "    remaining -= len(payload)\n",
        "\n",
        "\n",
        "df_stream = pd.DataFrame(all_rows)\n",
        "\n",
        "\n",
        "df_stream.rename(columns={\n",
        "    \"timestamp\": \"Timestamp\",\n",
        "    \"open\": \"Open\",\n",
        "    \"high\": \"High\",\n",
        "    \"low\": \"Low\",\n",
        "    \"close\": \"Close\",\n",
        "    \"volume\": \"Volume\"\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "df_stream[\"Timestamp\"] = pd.to_datetime(\n",
        "    df_stream[\"Timestamp\"].astype(int), unit=\"s\", utc=True\n",
        ").dt.tz_convert(\"Asia/Jakarta\").dt.tz_localize(None)\n",
        "\n",
        "\n",
        "for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
        "    df_stream[col] = pd.to_numeric(df_stream[col], errors=\"coerce\")\n",
        "\n",
        "\n",
        "df_stream.drop_duplicates(subset=[\"Timestamp\"], keep=\"first\", inplace=True)\n",
        "\n",
        "df_stream.sort_values(\"Timestamp\", inplace=True)\n",
        "\n",
        "if len(df_stream) > TOTAL_NEEDED:\n",
        "    df_stream = df_stream.tail(TOTAL_NEEDED)\n",
        "\n",
        "\n",
        "output_path = \"/btcusd_bitstamp_4h_9000.csv\"\n",
        "df_stream.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Data berhasil diexport ke: {output_path}\")\n",
        "print(df_stream.head())\n",
        "print(\"Total baris:\", len(df_stream))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Historis\n",
        "\n",
        "menghapus 30% dari data historis"
      ],
      "metadata": {
        "id": "_jWAAeAcchh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "input_path = \"/btc_4h.csv\"\n",
        "output_path = \"/btc_4h_minus_last9000.csv\"\n",
        "\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "before_len = len(df)\n",
        "drop_count = 9000 if before_len >= 9000 else before_len\n",
        "after_len = before_len - drop_count\n",
        "\n",
        "df_trim = df.iloc[:after_len]\n",
        "\n",
        "df_trim.to_csv(output_path, index=False)\n",
        "\n",
        "stats = {\n",
        "    \"rows_before\": before_len,\n",
        "    \"rows_dropped\": drop_count,\n",
        "    \"rows_after\": after_len,\n",
        "    \"columns\": list(df.columns),\n",
        "}\n",
        "\n",
        "if \"Timestamp\" in df_trim.columns and not df_trim.empty:\n",
        "    stats[\"timestamp_min\"] = str(df_trim[\"Timestamp\"].iloc[0])\n",
        "    stats[\"timestamp_max\"] = str(df_trim[\"Timestamp\"].iloc[-1])\n",
        "else:\n",
        "    stats[\"timestamp_min\"] = None\n",
        "    stats[\"timestamp_max\"] = None\n",
        "\n",
        "stats, os.path.exists(output_path), output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f002-yfLYORX",
        "outputId": "a0b54788-4ff4-491b-baaa-15ce8df19b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'rows_before': 30112,\n",
              "  'rows_dropped': 9000,\n",
              "  'rows_after': 21112,\n",
              "  'columns': ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'],\n",
              "  'timestamp_min': '2012-01-01 16:00:00+07:00',\n",
              "  'timestamp_max': '2021-08-20 04:00:00+07:00'},\n",
              " True,\n",
              " '/btc_4h_minus_last9000.csv')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merging both dataset\n",
        "\n",
        "menggbungkan 70% data historis dengan 30% data hasil scrapping\n"
      ],
      "metadata": {
        "id": "p6j1VleBeOj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "old_path = \"/btc_4h_minus_last9000.csv\"      # dataset lama (setelah drop 6000 terakhir)\n",
        "new_path = \"/btcusd_bitstamp_4h_9000.csv\"    # 6000 candle terbaru (hasil scraping)\n",
        "out_path = \"/btc_4h_merged.csv\"\n",
        "\n",
        "\n",
        "def normalize_columns(df):\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    rename_map = {\n",
        "        \"timestamp\": \"Timestamp\",\n",
        "        \"open\": \"Open\",\n",
        "        \"high\": \"High\",\n",
        "        \"low\": \"Low\",\n",
        "        \"close\": \"Close\",\n",
        "        \"volume\": \"Volume\",\n",
        "    }\n",
        "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
        "    final_cols = [\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "    df = df[[c for c in final_cols if c in df.columns]]\n",
        "    return df\n",
        "\n",
        "\n",
        "def to_wib_naive(ts_series):\n",
        "    s = ts_series.astype(str)\n",
        "    def _convert_one(x):\n",
        "        try:\n",
        "            dt = pd.to_datetime(x)\n",
        "        except Exception:\n",
        "            return pd.NaT\n",
        "        if getattr(dt, \"tz\", None) is not None:\n",
        "            return dt.tz_convert(\"Asia/Jakarta\").tz_localize(None)\n",
        "        return dt\n",
        "    return s.apply(_convert_one)\n",
        "\n",
        "\n",
        "df_old = pd.read_csv(old_path, dtype={\"Timestamp\": \"string\"})\n",
        "df_new = pd.read_csv(new_path, dtype={\"Timestamp\": \"string\"})\n",
        "\n",
        "df_old = normalize_columns(df_old)\n",
        "df_new = normalize_columns(df_new)\n",
        "\n",
        "df_old[\"Timestamp\"] = to_wib_naive(df_old[\"Timestamp\"])\n",
        "df_new[\"Timestamp\"] = to_wib_naive(df_new[\"Timestamp\"])\n",
        "\n",
        "for df in (df_old, df_new):\n",
        "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "\n",
        "df_merged = pd.concat([df_old, df_new], ignore_index=True)\n",
        "\n",
        "df_merged.drop_duplicates(subset=[\"Timestamp\"], keep=\"last\", inplace=True)\n",
        "\n",
        "df_merged.sort_values(\"Timestamp\", inplace=True)\n",
        "\n",
        "\n",
        "df_merged.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"Output disimpan ke:\", out_path)\n",
        "print(\"Total baris:\", len(df_merged))\n",
        "if not df_merged.empty:\n",
        "    print(\"Range waktu:\", df_merged[\"Timestamp\"].iloc[0], \"→\", df_merged[\"Timestamp\"].iloc[-1])\n",
        "\n",
        "print(df_merged[\"Timestamp\"].diff().value_counts().head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2zdbHdkYtmM",
        "outputId": "f67766db-846b-494c-97ba-d1bd62eec39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output disimpan ke: /btc_4h_merged.csv\n",
            "Total baris: 30112\n",
            "Range waktu: 2012-01-01 16:00:00 → 2025-11-07 19:00:00\n",
            "Timestamp\n",
            "0 days 04:00:00     30110\n",
            "40 days 19:00:00        1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}